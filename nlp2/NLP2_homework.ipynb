{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NLP2-homework",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### NLP2_1 https://www.hackerrank.com/challenges/detect-the-email-addresses/problem?isFullScreen=true"
   ],
   "metadata": {
    "id": "OYZRf1ncz-sT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "\n",
    "def print_matches(pattern, file_path):\n",
    "    matches = set()\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            matches.update(regex.findall(pattern, line))\n",
    "    for match in matches:\n",
    "        print(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interviewstreet@hackerrank.com\n",
      "hackers@hackerrank.com\n",
      "product@hackerrank.com\n"
     ]
    }
   ],
   "source": [
    "s = '[a-zA-Z0-9_.]*'\n",
    "pattern = regex.compile(rf'{s}@{s}')\n",
    "print_matches(pattern, \"./text1.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLP2_2 https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ],
   "metadata": {
    "id": "jKzbIfdq0CKr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askoxford.com\n",
      "bnsf.com\n",
      "hydrogencarsnow.com\n",
      "web.archive.org\n",
      "mrvc.indianrail.gov.in\n"
     ]
    }
   ],
   "source": [
    "pattern = regex.compile(r'https?:\\/\\/(?:www.|ww2.)?([^\\/]*)\\/')\n",
    "print_matches(pattern, \"./text2.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)\n",
    "#### https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments\n",
    "#### Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n",
    "\n"
   ],
   "metadata": {
    "id": "b5DQQnoU1bXY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "with open(\"./labeled.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    data = [comment for [comment, toxic] in reader]\n",
    "\n",
    "data = data[1:]"
   ],
   "metadata": {
    "id": "0BRC1-k81pIW"
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Верблюдов', '-', 'то', 'за', 'что', '?', 'Дебилы', ',', 'бл', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "data_tok = []\n",
    "for sentence in data:\n",
    "    data_tok.append(tokenizer.tokenize(sentence.lower()))\n",
    "\n",
    "print(tokenizer.tokenize(data[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 33932), ('.', 26863), ('и', 12684), ('в', 11974), ('не', 10301), ('-', 7906), ('на', 7003), ('что', 5986), ('а', 5008), ('?', 4395)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemms = []\n",
    "for words in data_tok:\n",
    "    gen = (lemmatizer.lemmatize(word) for word in words)\n",
    "    lemms.extend(gen)\n",
    "\n",
    "counter = Counter(lemms)\n",
    "dictionary = dict(counter)\n",
    "print(counter.most_common(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"russian\")\n",
    "stemmed = []\n",
    "\n",
    "for words in data_tok:\n",
    "    d = [stemmer.stem(word) for word in words]\n",
    "    stemmed.extend(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words:  68638\n",
      "after lemmatization:  68602\n",
      "after stemming:  33642\n",
      "\"и\" встречается 12684 раз\n"
     ]
    }
   ],
   "source": [
    "flat_list = [word for words in data_tok for word in words]\n",
    "\n",
    "print('total number of words: ', len(set(flat_list)))\n",
    "print('after lemmatization: ', len(set(lemms)))\n",
    "print('after stemming: ', len(set(stemmed)))\n",
    "\n",
    "vocabulary = dict(Counter(flat_list))  #словарь для BoW\n",
    "\n",
    "#special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "#for key in special_char:\n",
    "#          vocabulary.pop(key, None)\n",
    "\n",
    "word_to_search = 'и'\n",
    "print(f'\"{word_to_search}\" встречается {vocabulary[word_to_search]} раз')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 элементов словаря:\n",
      "верблюдов\n",
      "-\n",
      "то\n",
      "за\n",
      "что\n",
      "?\n",
      "дебилы\n",
      ",\n",
      "бл\n",
      "...\n",
      "хохлы\n",
      "это\n",
      "отдушина\n",
      "затюканого\n",
      "россиянина\n",
      "мол\n",
      "вон\n",
      "а\n",
      "у\n",
      "хохлов\n",
      "еще\n",
      "хуже\n",
      "Токены комментария:['верблюдов', '-', 'то', 'за', 'что', '?', 'дебилы', ',', 'бл', '...']\n",
      "Первые 20 компонент вектора слов, соответствующего комментарию:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    return [tokens.count(w) for w in vocabulary]\n",
    "\n",
    "\n",
    "n = 20\n",
    "k = 20\n",
    "print(f'Первые {n} элементов словаря:')\n",
    "for i, (key, value) in enumerate(vocabulary.items()):\n",
    "    print(key)\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "comment = data_tok[0]\n",
    "print(f'Токены комментария:{comment}')\n",
    "bow_vector = vectorize(comment)\n",
    "print(f'Первые {k} компонент вектора слов, соответствующего комментарию:{bow_vector[:k]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}